{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35379cb",
   "metadata": {},
   "source": [
    "## Noise Resilience Benchmarking of Hybrid Quantum-Classical Face Verification Under NISQ Constraints​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2567911",
   "metadata": {},
   "source": [
    "### Step 1：Image Filtering - from preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f9adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & config ---\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "ROOT = Path(\"./\")\n",
    "DATA_A = ROOT / \"data\" / \"positive\"\n",
    "DATA_B = ROOT / \"data\" / \"negative\"\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91941a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "train_root = ROOT / \"_bin_dataset\" / \"train\"\n",
    "val_root   = ROOT / \"_bin_dataset\" / \"val\"\n",
    "\n",
    "def build_split(srcA, srcB, train_ratio=0.8, limit_per_class=300):\n",
    "    import shutil\n",
    "    for p in [train_root, val_root]:\n",
    "        if p.exists():\n",
    "            shutil.rmtree(p)\n",
    "    for p in [train_root/srcA.name, train_root/srcB.name, val_root/srcA.name, val_root/srcB.name]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def list_imgs(d: Path):\n",
    "        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}\n",
    "        return [p for p in d.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "    A = list_imgs(srcA)\n",
    "    B = list_imgs(srcB)\n",
    "    random.shuffle(A); random.shuffle(B)\n",
    "    A = A[:min(limit_per_class, len(A))]\n",
    "    B = B[:min(limit_per_class, len(B))]\n",
    "\n",
    "    kA = int(len(A)*train_ratio)\n",
    "    kB = int(len(B)*train_ratio)\n",
    "\n",
    "    for src in A[:kA]: shutil.copy(src, train_root/srcA.name/src.name)\n",
    "    for src in A[kA:]: shutil.copy(src, val_root/srcA.name/src.name)\n",
    "    for src in B[:kB]: shutil.copy(src, train_root/srcB.name/src.name)\n",
    "    for src in B[kB:]: shutil.copy(src, val_root/srcB.name/src.name)\n",
    "\n",
    "build_split(DATA_A, DATA_B)\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_root, transform=tfm)\n",
    "val_ds   = datasets.ImageFolder(val_root,   transform=tfm)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_backbone = ROOT / \"outputs\" / \"resnet18_backbone_only.pt\"\n",
    "if not ckpt_backbone.exists():\n",
    "    alt = ROOT / \"resnet18_finetuned.pt\"\n",
    "    assert alt.exists(), \"outputs/resnet18_backbone_only.pt or resnet18_finetuned.pt\"\n",
    "    ckpt_backbone = alt\n",
    "\n",
    "backbone = resnet18(weights=None)\n",
    "state = torch.load(ckpt_backbone, map_location=\"cpu\")\n",
    "if isinstance(state, dict) and \"state_dict\" in state:\n",
    "    state = state[\"state_dict\"]\n",
    "_ = backbone.load_state_dict(state, strict=False)\n",
    "\n",
    "in_feats = backbone.fc.in_features  # 512\n",
    "backbone.fc = nn.Identity()\n",
    "backbone.to(device).eval()\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5acac",
   "metadata": {},
   "source": [
    "## L512-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe50ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L512to4(nn.Module):\n",
    "    def __init__(self, in_dim=512, hidden_dim=4):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):  # z: [B,512]\n",
    "        return self.act(self.fc(z))  # [B,4]\n",
    "\n",
    "proj = L512to4(in_dim=in_feats, hidden_dim=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85465ac",
   "metadata": {},
   "source": [
    "## Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "n_qubits = 4\n",
    "n_layers = 6\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "def entangle_ladder():\n",
    "\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_block(x, weights):\n",
    "    \"\"\"\n",
    "    x: [4] classical input from L512→4\n",
    "    weights: [6, 4] quantum parameters per layer per qubit\n",
    "    \"\"\"\n",
    "    for q in range(n_qubits):\n",
    "        qml.Hadamard(wires=q)\n",
    "        qml.RY(pnp.pi * x[q] / 2.0, wires=q)\n",
    "\n",
    "    for l in range(n_layers):\n",
    "        for q in range(n_qubits):\n",
    "            qml.RY(weights[l, q], wires=q)\n",
    "        entangle_ladder()\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(q)) for q in range(n_qubits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# QuantumLayer: input [B,4] → output [B,4]\n",
    "#############################################\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        w0 = 0.01 * torch.randn(n_layers, n_qubits)\n",
    "        self.weights = nn.Parameter(w0)\n",
    "\n",
    "    def forward(self, x4_batch):\n",
    "        outs = []\n",
    "        for i in range(x4_batch.shape[0]):\n",
    "            y = quantum_block(x4_batch[i], self.weights)  # ← outputs are float64\n",
    "            y = torch.stack(y)                            # shape [4], still float64\n",
    "            outs.append(y)\n",
    "        zq = torch.stack(outs, dim=0)                     # [B,4]\n",
    "\n",
    "        zq = zq.to(torch.float32)\n",
    "\n",
    "        return zq\n",
    "\n",
    "q_layer = QuantumLayer().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224b8a0",
   "metadata": {},
   "source": [
    "## L4-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L4to2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(4, 2)\n",
    "    def forward(self, z4):\n",
    "        return self.fc(z4)\n",
    "\n",
    "head = L4to2().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037fac0",
   "metadata": {},
   "source": [
    "## L+Q+L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Hybrid model = backbone → L512→4 → Q → L4→2\n",
    "#############################################\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, backbone, proj, q_layer, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = proj\n",
    "        self.q_layer = q_layer\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        with torch.no_grad():\n",
    "            z512 = self.backbone(imgs)\n",
    "\n",
    "        x4 = self.proj(z512)            # [B,4]\n",
    "        zq = self.q_layer(x4)           # [B,4], quantum output\n",
    "        logits = self.head(zq)          # [B,2]\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = HybridModel(backbone, proj, q_layer, head).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f022e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train 0.3837/0.929 | Val 0.2993/0.983\n",
      "[2] Train 0.2883/0.975 | Val 0.2797/0.975\n",
      "[3] Train 0.2639/0.981 | Val 0.2609/0.975\n",
      "[4] Train 0.2424/0.983 | Val 0.2417/0.975\n",
      "[5] Train 0.2290/0.981 | Val 0.2303/0.975\n"
     ]
    }
   ],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": proj.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": q_layer.parameters(), \"lr\": 1e-2},\n",
    "    {\"params\": head.parameters(), \"lr\": 1e-3},\n",
    "])\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(imgs)\n",
    "            loss = crit(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "# ----------------- Run training -----------------\n",
    "for ep in range(1, 6):\n",
    "    trL, trA = run_epoch(train_loader, True)\n",
    "    vaL, vaA = run_epoch(val_loader, False)\n",
    "    print(f\"[{ep}] Train {trL:.4f}/{trA:.3f} | Val {vaL:.4f}/{vaA:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e3a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to: artifacts\\hybrid_qml_best.pt\n"
     ]
    }
   ],
   "source": [
    "import os, torch, json\n",
    "from datetime import datetime\n",
    "\n",
    "SAVE_DIR = \"artifacts\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "num_classes = getattr(model, \"num_classes\", None) or 2\n",
    "class_names = None\n",
    "for cand in [\"train_ds\", \"dataset\", \"train_set\"]:\n",
    "    if cand in globals() and hasattr(globals()[cand], \"classes\"):\n",
    "        class_names = list(getattr(globals()[cand], \"classes\"))\n",
    "        break\n",
    "if class_names is None:\n",
    "    class_names = [f\"class{i}\" for i in range(num_classes)]\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "ckpt = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"meta\": {\n",
    "        \"class_names\": class_names,\n",
    "        \"img_size\": 224,\n",
    "        \"mean\": IMAGENET_MEAN,\n",
    "        \"std\": IMAGENET_STD,\n",
    "        \"model_hparams\": {\n",
    "            \"proj_dim\": 4,\n",
    "            \"n_qubits\": 4,\n",
    "            \"n_layers\": 6, \n",
    "            \"num_classes\": num_classes\n",
    "        },\n",
    "        \"notes\": \"ResNet18 backbone + 512->4 tanh + 4q VQC + 4->2 head\"\n",
    "    },\n",
    "    \"versions\": {\n",
    "        \"torch\": torch.__version__\n",
    "    },\n",
    "    \"saved_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "\n",
    "save_path = os.path.join(SAVE_DIR, \"hybrid_qml_best.pt\")\n",
    "torch.save(ckpt, save_path)\n",
    "print(f\"Saved checkpoint to: {save_path}\")\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"hybrid_qml_best.meta.json\"), \"w\") as f:\n",
    "    json.dump(ckpt[\"meta\"], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "089bc65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] Saved model state_dict to: best_model.pt\n",
      "[load] Loaded weights and rebuilt hybrid model for inference.\n"
     ]
    }
   ],
   "source": [
    "# === Save + reload + single-image predict (matches this notebook's paths/arch) ===\n",
    "import os, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Reuse ROOT if defined; otherwise default to project root\n",
    "ROOT = Path(globals().get(\"ROOT\", Path(\"./\")))\n",
    "SAVE_PATH = ROOT / \"best_model.pt\"\n",
    "\n",
    "# 1) Save the current trained model (state_dict)\n",
    "if \"model\" in globals():\n",
    "    torch.save(model.state_dict(), SAVE_PATH)\n",
    "    print(f\"[save] Saved model state_dict to: {SAVE_PATH}\")\n",
    "else:\n",
    "    print(\"[save] No 'model' in memory; will just load from disk below.\")\n",
    "\n",
    "# 2) Rebuild the SAME hybrid architecture and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Require the same classes that exist earlier in this notebook\n",
    "required = [\"L512to4\", \"QuantumLayer\", \"L4to2\", \"HybridModel\"]\n",
    "missing = [cls for cls in required if cls not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Please run the earlier cells to define {missing} before running this cell.\")\n",
    "\n",
    "# Backbone (fc removed to yield 512-d features)\n",
    "backbone = resnet18(weights=None)\n",
    "backbone.fc = torch.nn.Identity()\n",
    "\n",
    "proj = L512to4(in_dim=512, hidden_dim=4)\n",
    "q_layer = QuantumLayer()\n",
    "head = L4to2()\n",
    "\n",
    "model_inf = HybridModel(backbone, proj, q_layer, head).to(device)\n",
    "state = torch.load(SAVE_PATH, map_location=device)\n",
    "model_inf.load_state_dict(state, strict=False)\n",
    "model_inf.eval()\n",
    "print(\"[load] Loaded weights and rebuilt hybrid model for inference.\")\n",
    "\n",
    "# 3) Preprocess (same as training/ImageNet stats)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "infer_tf = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# 4) Predict ONE image\n",
    "@torch.no_grad()\n",
    "def predict_one_with_model(img_path: str, model_for_infer: torch.nn.Module, class_names=None):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = infer_tf(img).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model_for_infer(x)\n",
    "\n",
    "    # binary or multi-class friendly\n",
    "    if logits.shape[1] == 1:\n",
    "        p1 = torch.sigmoid(logits[:, 0])\n",
    "        probs = torch.stack([1 - p1, p1], dim=1)\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    probs = probs.squeeze(0).cpu()\n",
    "    pred_idx = int(probs.argmax().item())\n",
    "\n",
    "    # reuse dataset class names if available\n",
    "    if class_names is None:\n",
    "        names = None\n",
    "        for cand in [\"train_ds\", \"val_ds\", \"dataset\", \"train_set\"]:\n",
    "            if cand in globals() and hasattr(globals()[cand], \"classes\"):\n",
    "                names = list(getattr(globals()[cand], \"classes\"))\n",
    "                break\n",
    "        if names is None or len(names) != probs.numel():\n",
    "            names = [f\"class{i}\" for i in range(probs.numel())]\n",
    "    else:\n",
    "        names = class_names if len(class_names) == probs.numel() else [f\"class{i}\" for i in range(probs.numel())]\n",
    "\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Prediction: {names[pred_idx]} (idx={pred_idx})\")\n",
    "    for i, p in enumerate(probs.tolist()):\n",
    "        print(f\"  {names[i]:>12s}: {p:.4f}\")\n",
    "\n",
    "    return {\"label\": names[pred_idx], \"index\": pred_idx, \"probs\": probs.tolist()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
