{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, copy\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Quantum Imports ---\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as pnp\n",
    "except ImportError:\n",
    "    print(\"Warning: PennyLane not found. Quantum model functionality will fail.\")\n",
    "    qml = None\n",
    "    pnp = None\n",
    "\n",
    "# ======================================================================\n",
    "# --- 1. Common Definitions\n",
    "# ======================================================================\n",
    "\n",
    "def device_auto() -> torch.device:\n",
    "    \"\"\"Detects the best available device (CUDA, MPS, or CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = device_auto()\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# --- Paths ---\n",
    "ROOT = Path(\"./\")\n",
    "CLASSICAL_MODEL_PATH = ROOT / \"outputs\" / \"resnet18_finetuned_with_head.pt\"\n",
    "QUANTUM_SINGLE_MODEL_PATH = ROOT / \"artifacts\" / \"hybrid_qml_best.pt\"\n",
    "ENSEMBLE_DIR = ROOT / \"artifacts\" / \"ensemble\"\n",
    "TEST_DIR = ROOT / \"final_test_full\"\n",
    "N_MODELS = 5\n",
    "\n",
    "# --- Class Lists ---\n",
    "CLASSES_CLASSICAL = ['Negative', 'Positive']\n",
    "CLASSES_QUANTUM = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e445261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# --- 2. Classical Model Definitions\n",
    "# ======================================================================\n",
    "\n",
    "def load_classical_model(model_path: str, num_classes: int) -> nn.Module:\n",
    "    \"\"\"Loads the trained classical model from checkpoint\"\"\"\n",
    "    model = models.resnet18(weights=None)\n",
    "    in_feats = model.fc.in_features\n",
    "    if num_classes == 2:\n",
    "        model.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, 1))\n",
    "    else:\n",
    "        model.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "    except Exception:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint.get('state_dict', checkpoint))\n",
    "        \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "classical_infer_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_classical(model: nn.Module, img_tensor: torch.Tensor, classes: List[str]) -> dict:\n",
    "    \"\"\"Predicts using the classical model from a pre-processed tensor\"\"\"\n",
    "    logits = model(img_tensor)\n",
    "    p = torch.sigmoid(logits).item()\n",
    "    pred_idx = int(p >= 0.5)\n",
    "    return {\n",
    "        \"pred_idx\": pred_idx, \n",
    "        \"pred_class\": classes[pred_idx], \n",
    "        \"prob_positive\": float(p),\n",
    "        \"prob_negative\": float(1 - p)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# --- 3. All Quantum Model Definitions (Corrected)\n",
    "# ======================================================================\n",
    "\n",
    "if qml:\n",
    "    from math import pi as PI\n",
    "    n_qubits = 4 # All models use 4 qubits\n",
    "\n",
    "    # --- 3a. Definitions for NEW Diverse Ensemble Models ---\n",
    "    def load_frozen_backbone() -> nn.Module:\n",
    "        ckpt_backbone = ROOT / \"outputs\" / \"resnet18_backbone_only.pt\"\n",
    "        if not ckpt_backbone.exists():\n",
    "            ckpt_backbone = ROOT / \"resnet18_finetuned.pt\"\n",
    "        backbone = models.resnet18(weights=None)\n",
    "        state = torch.load(ckpt_backbone, map_location=\"cpu\")\n",
    "        if isinstance(state, dict) and \"state_dict\" in state:\n",
    "            state = state[\"state_dict\"]\n",
    "        _ = backbone.load_state_dict(state, strict=False)\n",
    "        backbone.fc = nn.Identity()\n",
    "        backbone.eval().to(device)\n",
    "        for p in backbone.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        return backbone\n",
    "\n",
    "    BACKBONE = load_frozen_backbone()\n",
    "\n",
    "    class L512to4(nn.Module):\n",
    "        def __init__(self, in_dim=512, hidden_dim=4):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(in_dim, hidden_dim)\n",
    "            self.act = nn.Tanh()\n",
    "        def forward(self, z):\n",
    "            return self.act(self.fc(z))\n",
    "\n",
    "    def make_entangler(kind: str):\n",
    "        if kind == \"ladder\": return [(1,2), (0,1), (2,3)]\n",
    "        if kind == \"ring\": return [(0,1), (1,2), (2,3), (3,0)]\n",
    "        pairs = [(0,1),(1,2),(2,3),(0,2),(1,3)]\n",
    "        random.shuffle(pairs)\n",
    "        return pairs[:4]\n",
    "\n",
    "    class QuantumLayer_Diverse(nn.Module):\n",
    "        def __init__(self, depth, pairs, shots=None):\n",
    "            super().__init__()\n",
    "            self.depth = int(depth)\n",
    "            self.pairs = pairs\n",
    "            self.weights = nn.Parameter(0.01 * torch.randn(self.depth, n_qubits))\n",
    "            self.dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "\n",
    "            def circuit(x, w):\n",
    "                for q in range(n_qubits):\n",
    "                    qml.Hadamard(wires=q)\n",
    "                    qml.RY(PI * x[q] / 2.0, wires=q)\n",
    "                for l in range(self.depth):\n",
    "                    for q in range(n_qubits):\n",
    "                        qml.RY(w[l, q], wires=q)\n",
    "                    for a,b in self.pairs:\n",
    "                        qml.CNOT(wires=[a,b])\n",
    "                return [qml.expval(qml.PauliZ(q)) for q in range(n_qubits)]\n",
    "\n",
    "            self.qnode = qml.QNode(circuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
    "\n",
    "        def forward(self, x4_batch: torch.Tensor) -> torch.Tensor:\n",
    "            outs = []\n",
    "            for i in range(x4_batch.shape[0]):\n",
    "                y = self.qnode(x4_batch[i], self.weights)\n",
    "                if not isinstance(y, torch.Tensor):\n",
    "                    y = torch.stack(y)\n",
    "                outs.append(y)\n",
    "            return torch.stack(outs, dim=0).to(torch.float32)\n",
    "\n",
    "    class L4to2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(4, 2)\n",
    "        def forward(self, z4):\n",
    "            return self.fc(z4)\n",
    "\n",
    "    class HybridModel_Diverse(nn.Module):\n",
    "        def __init__(self, backbone, proj, q_layer, head):\n",
    "            super().__init__()\n",
    "            self.backbone = backbone\n",
    "            self.proj = proj\n",
    "            self.q_layer = q_layer\n",
    "            self.head = head\n",
    "        def forward(self, x):\n",
    "            if x.dim() == 2 and x.size(1) == 512:\n",
    "                z512 = x.to(device)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    z512 = self.backbone(x)\n",
    "            x4 = self.proj(z512)\n",
    "            zq = self.q_layer(x4)\n",
    "            logits = self.head(zq)\n",
    "            return logits\n",
    "\n",
    "    def create_new_diverse_model(depth, pairs):\n",
    "        \"\"\"Helper from ensamble.ipynb (cell 32)\"\"\"\n",
    "        local_backbone = copy.deepcopy(BACKBONE)\n",
    "        proj = L512to4(512, n_qubits)\n",
    "        q_layer = QuantumLayer_Diverse(depth=depth, pairs=pairs)\n",
    "        head = L4to2()\n",
    "        return HybridModel_Diverse(local_backbone, proj, q_layer, head).to(device)\n",
    "\n",
    "    # --- 3b. Definitions for OLD Single Model ---\n",
    "    \n",
    "    simple_dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    simple_n_layers = 6 # The original model had a fixed depth of 6\n",
    "\n",
    "    def simple_entangle_ladder():\n",
    "        qml.CNOT(wires=[1, 2])\n",
    "        qml.CNOT(wires=[0, 1])\n",
    "        qml.CNOT(wires=[2, 3])\n",
    "\n",
    "    @qml.qnode(simple_dev, interface=\"torch\")\n",
    "    def simple_quantum_block(x, weights):\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "            qml.RY(pnp.pi * x[q] / 2.0, wires=q)\n",
    "        for l in range(simple_n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RY(weights[l, q], wires=q)\n",
    "            simple_entangle_ladder()\n",
    "        return [qml.expval(qml.PauliZ(q)) for q in range(n_qubits)]\n",
    "\n",
    "    class QuantumLayer_Simple(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            w0 = 0.01 * torch.randn(simple_n_layers, n_qubits)\n",
    "            self.weights = nn.Parameter(w0)\n",
    "        def forward(self, x4_batch):\n",
    "            outs = []\n",
    "            for i in range(x4_batch.shape[0]):\n",
    "                y = simple_quantum_block(x4_batch[i], self.weights)\n",
    "                y = torch.stack(y)\n",
    "                outs.append(y)\n",
    "            zq = torch.stack(outs, dim=0)\n",
    "            return zq.to(torch.float32)\n",
    "\n",
    "    class HybridModel_Simple(nn.Module):\n",
    "        def __init__(self, backbone, proj, q_layer, head):\n",
    "            super().__init__()\n",
    "            self.backbone = backbone\n",
    "            self.proj = proj\n",
    "            self.q_layer = q_layer\n",
    "            self.head = head\n",
    "        def forward(self, imgs):\n",
    "            with torch.no_grad():\n",
    "                z512 = self.backbone(imgs)\n",
    "            x4 = self.proj(z512)\n",
    "            zq = self.q_layer(x4)\n",
    "            logits = self.head(zq)\n",
    "            return logits\n",
    "\n",
    "    def load_quantum_single_model(model_path: str):\n",
    "        \"\"\"Loads the original single HybridModel\"\"\"\n",
    "        backbone_simple = copy.deepcopy(BACKBONE)\n",
    "        proj_simple = L512to4(in_dim=512, hidden_dim=n_qubits)\n",
    "        q_layer_simple = QuantumLayer_Simple() # Use the simple layer\n",
    "        head_simple = L4to2()\n",
    "        \n",
    "        model_inf = HybridModel_Simple(backbone_simple, proj_simple, q_layer_simple, head_simple).to(device)\n",
    "        \n",
    "        ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        model_inf.load_state_dict(ckpt[\"state_dict\"])\n",
    "        model_inf.eval()\n",
    "        class_names = ckpt.get(\"meta\", {}).get(\"class_names\", CLASSES_QUANTUM)\n",
    "        return model_inf, class_names\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_quantum_single(model: nn.Module, img_tensor: torch.Tensor, class_names: List[str]) -> dict:\n",
    "        \"\"\"Predicts using the single quantum model\"\"\"\n",
    "        logits = model(img_tensor)\n",
    "        probs = F.softmax(logits, dim=1).squeeze(0).cpu()\n",
    "        pred_idx = int(probs.argmax().item())\n",
    "        return {\n",
    "            \"pred_idx\": pred_idx,\n",
    "            \"pred_class\": class_names[pred_idx],\n",
    "            \"prob_positive\": probs[1].item(),\n",
    "            \"prob_negative\": probs[0].item(),\n",
    "        }\n",
    "    # --- 3c. Loaders and Predictors for Ensembles ---\n",
    "\n",
    "    def load_base_models():\n",
    "        \"\"\"Loads the new diverse models, reading their configs\"\"\"\n",
    "        models, temps = [], []\n",
    "        for k in range(N_MODELS):\n",
    "            path = ENSEMBLE_DIR / f\"model_{k}.pt\"\n",
    "            cfg_path = ENSEMBLE_DIR / f\"model_{k}_cfg.pt\"\n",
    "            tpath = ENSEMBLE_DIR / f\"model_{k}_temp.pt\"\n",
    "\n",
    "            if not path.exists():\n",
    "                print(f\"Warning: Missing weights file {path}, skipping model {k}.\")\n",
    "                continue\n",
    "            \n",
    "            # 1) Load weights file *FIRST* to get true depth\n",
    "            sd = torch.load(path, map_location=\"cpu\")\n",
    "            try:\n",
    "                depth = sd[\"q_layer.weights\"].shape[0]\n",
    "            except KeyError:\n",
    "                print(f\"Warning: 'q_layer.weights' not in {path}, skipping model {k}.\")\n",
    "                continue\n",
    "            \n",
    "            # 2) Load config file to get pairs\n",
    "            pairs = None\n",
    "            if cfg_path.exists():\n",
    "                cfg = torch.load(cfg_path, map_location=\"cpu\")\n",
    "                pairs = cfg.get(\"pairs\", None)\n",
    "            \n",
    "            if pairs is None:\n",
    "                print(f\"Warning: No config for model {k}. Guessing 'ladder' entangler.\")\n",
    "                pairs = make_entangler(\"ladder\")\n",
    "\n",
    "            # 3) Instantiate model with correct depth/pairs\n",
    "            m = create_new_diverse_model(depth=depth, pairs=pairs)\n",
    "            \n",
    "            # 4) Load weights\n",
    "            m.load_state_dict(sd, strict=True)\n",
    "            m.eval().to(device)\n",
    "            models.append(m)\n",
    "\n",
    "            # 5) Load temperature\n",
    "            if tpath.exists():\n",
    "                temps.append(float(torch.load(tpath, map_location=\"cpu\")[\"temperature\"]))\n",
    "            else:\n",
    "                temps.append(1.0)\n",
    "                \n",
    "        return models, temps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_soft_vote(x, models_list, temps_list, class_names):\n",
    "        \"\"\"x: image tensor [1,3,224,224]\"\"\"\n",
    "        probs = []\n",
    "        for m, t in zip(models_list, temps_list):\n",
    "            logits = m(x.to(device)) / max(t, 1e-3) # Apply temperature\n",
    "            probs.append(F.softmax(logits, dim=1))\n",
    "        avg = torch.mean(torch.stack(probs), dim=0).squeeze(0).cpu()\n",
    "        pred_idx = int(avg.argmax().item())\n",
    "        return {\n",
    "            \"pred_idx\": pred_idx, \n",
    "            \"pred_class\": class_names[pred_idx],\n",
    "            \"prob_positive\": avg[1].item(),\n",
    "            \"prob_negative\": avg[0].item(),\n",
    "        }\n",
    "\n",
    "    class StackedEnsemble(nn.Module):\n",
    "        def __init__(self, bases, temps, meta):\n",
    "            super().__init__()\n",
    "            self.bases = nn.ModuleList(bases)\n",
    "            for m in self.bases:\n",
    "                for p in m.parameters(): p.requires_grad = False\n",
    "            self.temps = [float(t) for t in temps]\n",
    "            self.meta = meta.eval()\n",
    "        @torch.no_grad()\n",
    "        def forward(self, x):\n",
    "            feats = []\n",
    "            for m, t in zip(self.bases, self.temps):\n",
    "                l = m(x.to(device)) / max(t, 1e-3) # Calibrated logits\n",
    "                feats.append(l)\n",
    "            meta_in = torch.cat(feats, dim=1) # [B, 10]\n",
    "            return self.meta(meta_in)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_stacked(x, stacked_model, class_names):\n",
    "        final_logits = stacked_model(x)\n",
    "        final_probs = F.softmax(final_logits, dim=1).squeeze(0).cpu()\n",
    "        pred_idx = int(final_probs.argmax().item())\n",
    "        return {\n",
    "            \"pred_idx\": pred_idx, \n",
    "            \"pred_class\": class_names[pred_idx],\n",
    "            \"prob_positive\": final_probs[1].item(),\n",
    "            \"prob_negative\": final_probs[0].item(),\n",
    "        }\n",
    "\n",
    "else:\n",
    "    print(\"Skipping All Quantum Model definitions.\")\n",
    "\n",
    "# Global transforms\n",
    "classical_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "quantum_tfms = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# --- 4. Main Evaluation Function (Modified)\n",
    "# ======================================================================\n",
    "from PIL import Image\n",
    "def run_evaluation():\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # --- 1. Load ALL Models ---\n",
    "    \n",
    "    # Load Classical\n",
    "    print(f\"Loading classical model from: {CLASSICAL_MODEL_PATH}\")\n",
    "    try:\n",
    "        classical_model = load_classical_model(CLASSICAL_MODEL_PATH, len(CLASSES_CLASSICAL))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Classical model file not found at {CLASSICAL_MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    # Load Quantum (Single)\n",
    "    print(f\"Loading quantum (single) model from: {QUANTUM_SINGLE_MODEL_PATH}\")\n",
    "    try:\n",
    "        q_single_model, q_single_classes = load_quantum_single_model(QUANTUM_SINGLE_MODEL_PATH)\n",
    "        q_single_classes = [c.lower() for c in q_single_classes]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Quantum (single) model file not found at {QUANTUM_SINGLE_MODEL_PATH}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading quantum single model: {e}\")\n",
    "        return\n",
    "        \n",
    "    # Load Quantum (Diverse Ensemble)\n",
    "    print(f\"Loading {N_MODELS} diverse ensemble base models from: {ENSEMBLE_DIR}\")\n",
    "    base_models, base_temps = load_base_models()\n",
    "    \n",
    "    if len(base_models) != N_MODELS:\n",
    "        print(f\"Error: Loaded {len(base_models)} but expected {N_MODELS}. Aborting.\")\n",
    "        print(\"Please check your training script and artifacts folder.\")\n",
    "        return\n",
    "\n",
    "    # Load Quantum (Stacking Meta-Learner)\n",
    "    print(f\"Loading stacking meta-learner...\")\n",
    "    meta_learner_path = ENSEMBLE_DIR / \"meta_learner.pt\"\n",
    "    try:\n",
    "        meta_learner = nn.Linear(2 * len(base_models), 2).to(device)\n",
    "        meta_learner.load_state_dict(torch.load(meta_learner_path, map_location=device))\n",
    "        meta_learner.eval()\n",
    "        stacked_model = StackedEnsemble(base_models, base_temps, meta_learner).to(device)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Meta-learner not found at {meta_learner_path}\")\n",
    "        print(\"Please run the diverse ensemble training notebook first.\")\n",
    "        return\n",
    "\n",
    "    print(\"All models loaded successfully.\")\n",
    "\n",
    "    # --- 2. Find Test Images ---\n",
    "    print(f\"Scanning for images in: {TEST_DIR}\")\n",
    "    if not TEST_DIR.exists():\n",
    "        print(f\"Error: Test directory not found at {TEST_DIR}\")\n",
    "        return\n",
    "        \n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    image_files = [p for p in TEST_DIR.rglob('*') if p.suffix.lower() in image_extensions]\n",
    "    print(f\"Found {len(image_files)} images to test.\")\n",
    "    if not image_files: return\n",
    "\n",
    "    # --- 3. Run Predictions and Collect Results ---\n",
    "    all_results = []\n",
    "    print(\"Running predictions on all models...\")\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Evaluating Images\"):\n",
    "        gt_label = img_path.parent.name.lower()\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            c_tensor = classical_tfms(img).unsqueeze(0).to(device)\n",
    "            q_tensor = quantum_tfms(img).unsqueeze(0).to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Skipping file {img_path} due to error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Run ALL predictors\n",
    "        c_res = predict_classical(classical_model, c_tensor, CLASSES_CLASSICAL)\n",
    "        q_single_res = predict_quantum_single(q_single_model, q_tensor, q_single_classes)\n",
    "        q_soft_res = predict_soft_vote(q_tensor, base_models, base_temps, q_single_classes)\n",
    "        q_stacked_res = predict_stacked(q_tensor, stacked_model, q_single_classes)\n",
    "\n",
    "        def process_res(res, gt):\n",
    "            pred_label = res['pred_class'].lower()\n",
    "            correct = (pred_label == gt)\n",
    "            prob_pos = res['prob_positive']\n",
    "            confidence = prob_pos if pred_label == 'positive' else (1 - prob_pos)\n",
    "            return pred_label, correct, prob_pos, confidence\n",
    "\n",
    "        c_label, c_correct, c_prob, c_conf = process_res(c_res, gt_label)\n",
    "        q_s_label, q_s_correct, q_s_prob, q_s_conf = process_res(q_single_res, gt_label)\n",
    "        q_v_label, q_v_correct, q_v_prob, q_v_conf = process_res(q_soft_res, gt_label)\n",
    "        q_t_label, q_t_correct, q_t_prob, q_t_conf = process_res(q_stacked_res, gt_label)\n",
    "\n",
    "        all_results.append({\n",
    "            \"path\": str(img_path.relative_to(TEST_DIR)),\n",
    "            \"gt\": gt_label,\n",
    "            \"c_pred\": c_label, \"c_correct\": c_correct, \"c_conf\": c_conf,\n",
    "            \"q_s_pred\": q_s_label, \"q_s_correct\": q_s_correct, \"q_s_conf\": q_s_conf,\n",
    "            \"q_v_pred\": q_v_label, \"q_v_correct\": q_v_correct, \"q_v_conf\": q_v_conf,\n",
    "            \"q_t_pred\": q_t_label, \"q_t_correct\": q_t_correct, \"q_t_conf\": q_t_conf,\n",
    "        })\n",
    "\n",
    "    # --- 4. Analyze and Report ---\n",
    "    total = len(all_results)\n",
    "    if total == 0:\n",
    "        print(\"No results to report.\")\n",
    "        return\n",
    "\n",
    "    c_total = sum(1 for r in all_results if r['c_correct'])\n",
    "    q_s_total = sum(1 for r in all_results if r['q_s_correct'])\n",
    "    q_v_total = sum(1 for r in all_results if r['q_v_correct'])\n",
    "    q_t_total = sum(1 for r in all_results if r['q_t_correct'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"       FINAL EVALUATION REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"\\n--- 1. Overall Accuracy ({total} images) ---\")\n",
    "    print(f\"Classical:             {c_total}/{total}  ({c_total/total:.2%})\")\n",
    "    print(f\"Quantum (Single VQC):  {q_s_total}/{total}  ({q_s_total/total:.2%})\")\n",
    "    print(f\"Quantum (Diverse Vote):  {q_v_total}/{total}  ({q_v_total/total:.2%})\")\n",
    "    print(f\"Quantum (Diverse Stack): {q_t_total}/{total}  ({q_t_total/total:.2%})\")\n",
    "\n",
    "    def mean_confidence(key_correct, key_conf):\n",
    "        corrects = [r[key_conf] for r in all_results if r[key_correct]]\n",
    "        wrongs = [r[key_conf] for r in all_results if not r[key_correct]]\n",
    "        mean_c = f\"{np.mean(corrects):.4f}\" if corrects else \"N/A\"\n",
    "        mean_w = f\"{np.mean(wrongs):.4f}\" if wrongs else \"N/A\"\n",
    "        return mean_c, mean_w\n",
    "\n",
    "    c_conf_c, c_conf_w = mean_confidence('c_correct', 'c_conf')\n",
    "    q_s_conf_c, q_s_conf_w = mean_confidence('q_s_correct', 'q_s_conf')\n",
    "    q_v_conf_c, q_v_conf_w = mean_confidence('q_v_correct', 'q_v_conf')\n",
    "    q_t_conf_c, q_t_conf_w = mean_confidence('q_t_correct', 'q_t_conf')\n",
    "\n",
    "    print(f\"\\n--- 2. Average Confidence ---\")\n",
    "    print(f\"  (Probability assigned to the *predicted* class)\")\n",
    "    print(f\"Model                  | Correct | Wrong\")\n",
    "    print(f\"-------------------------------------------\")\n",
    "    print(f\"Classical              | {c_conf_c:7} | {c_conf_w:7}\")\n",
    "    print(f\"Quantum (Single VQC)   | {q_s_conf_c:7} | {q_s_conf_w:7}\")\n",
    "    print(f\"Quantum (Diverse Vote) | {q_v_conf_c:7} | {q_v_conf_w:7}\")\n",
    "    print(f\"Quantum (Diverse Stack)| {q_t_conf_c:7} | {q_t_conf_w:7}\")\n",
    "\n",
    "    # Discrepancy Report (Comparing Classical vs. new best: Soft Vote)\n",
    "    print(f\"\\n--- 3. Discrepancy Report (Classical vs. Diverse Vote) ---\")\n",
    "    c_right_q_wrong = [r for r in all_results if r['c_correct'] and not r['q_v_correct']]\n",
    "    q_right_c_wrong = [r for r in all_results if not r['c_correct'] and r['q_v_correct']]\n",
    "    both_wrong = [r for r in all_results if not r['c_correct'] and not r['q_v_correct']]\n",
    "\n",
    "    print(f\"\\nClassical CORRECT, Vote WRONG ({len(c_right_q_wrong)}):\")\n",
    "    for r in c_right_q_wrong:\n",
    "        print(f\"  - {r['path']} (GT: {r['gt']})\")\n",
    "        print(f\"    Classical predicted: {r['c_pred']} ({r['c_conf']:.3f})\")\n",
    "        print(f\"    Vote predicted:      {r['q_v_pred']} ({r['q_v_conf']:.3f})\")\n",
    "\n",
    "    print(f\"\\nVote CORRECT, Classical WRONG ({len(q_right_c_wrong)}):\")\n",
    "    for r in q_right_c_wrong:\n",
    "        print(f\"  - {r['path']} (GT: {r['gt']})\")\n",
    "        print(f\"    Classical predicted: {r['c_pred']} ({r['c_conf']:.3f})\")\n",
    "        print(f\"    Vote predicted:      {r['q_v_pred']} ({r['q_v_conf']:.3f})\")\n",
    "\n",
    "    print(f\"\\nBoth WRONG ({len(both_wrong)}):\")\n",
    "    for r in both_wrong:\n",
    "        print(f\"  - {r['path']} (GT: {r['gt']})\")\n",
    "        print(f\"    Classical predicted: {r['c_pred']} ({r['c_conf']:.3f})\")\n",
    "        print(f\"    Vote predicted:      {r['q_v_pred']} ({r['q_v_conf']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"       END OF REPORT\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ac66d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading classical model from: outputs\\resnet18_finetuned_with_head.pt\n",
      "Loading quantum (single) model from: artifacts\\hybrid_qml_best.pt\n",
      "Loading 5 diverse ensemble base models from: artifacts\\ensemble\n",
      "Loading stacking meta-learner...\n",
      "All models loaded successfully.\n",
      "Scanning for images in: final_test_full\n",
      "Found 340 images to test.\n",
      "Running predictions on all models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|██████████| 340/340 [02:32<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "       FINAL EVALUATION REPORT\n",
      "========================================\n",
      "\n",
      "--- 1. Overall Accuracy (340 images) ---\n",
      "Classical:             307/340  (90.29%)\n",
      "Quantum (Single VQC):  315/340  (92.65%)\n",
      "Quantum (Diverse Vote):  320/340  (94.12%)\n",
      "Quantum (Diverse Stack): 320/340  (94.12%)\n",
      "\n",
      "--- 2. Average Confidence ---\n",
      "  (Probability assigned to the *predicted* class)\n",
      "Model                  | Correct | Wrong\n",
      "-------------------------------------------\n",
      "Classical              | 0.9787  | 0.8966 \n",
      "Quantum (Single VQC)   | 0.8082  | 0.7201 \n",
      "Quantum (Diverse Vote) | 0.9797  | 0.9046 \n",
      "Quantum (Diverse Stack)| 0.8534  | 0.7669 \n",
      "\n",
      "--- 3. Discrepancy Report (Classical vs. Diverse Vote) ---\n",
      "\n",
      "Classical CORRECT, Vote WRONG (5):\n",
      "  - negative\\569.jpg (GT: negative)\n",
      "    Classical predicted: negative (0.901)\n",
      "    Vote predicted:      positive (0.817)\n",
      "  - negative\\675.jpg (GT: negative)\n",
      "    Classical predicted: negative (0.745)\n",
      "    Vote predicted:      positive (0.846)\n",
      "  - positive\\16.jpg (GT: positive)\n",
      "    Classical predicted: positive (0.887)\n",
      "    Vote predicted:      negative (0.640)\n",
      "  - positive\\89.jpg (GT: positive)\n",
      "    Classical predicted: positive (0.858)\n",
      "    Vote predicted:      negative (0.850)\n",
      "  - positive\\97.jpg (GT: positive)\n",
      "    Classical predicted: positive (0.998)\n",
      "    Vote predicted:      negative (0.827)\n",
      "\n",
      "Vote CORRECT, Classical WRONG (18):\n",
      "  - negative\\584.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.831)\n",
      "    Vote predicted:      negative (0.963)\n",
      "  - negative\\585.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.991)\n",
      "    Vote predicted:      negative (0.735)\n",
      "  - negative\\586.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.996)\n",
      "    Vote predicted:      negative (0.978)\n",
      "  - negative\\587.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.997)\n",
      "    Vote predicted:      negative (0.951)\n",
      "  - negative\\588.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.847)\n",
      "    Vote predicted:      negative (0.979)\n",
      "  - negative\\589.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.638)\n",
      "    Vote predicted:      negative (0.978)\n",
      "  - negative\\70.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.507)\n",
      "    Vote predicted:      negative (0.974)\n",
      "  - negative\\733.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.623)\n",
      "    Vote predicted:      negative (0.963)\n",
      "  - negative\\738.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.624)\n",
      "    Vote predicted:      negative (0.977)\n",
      "  - negative\\742.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.969)\n",
      "    Vote predicted:      negative (0.974)\n",
      "  - positive\\123 (17).jpg (GT: positive)\n",
      "    Classical predicted: negative (0.880)\n",
      "    Vote predicted:      positive (0.982)\n",
      "  - positive\\21.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.876)\n",
      "    Vote predicted:      positive (0.972)\n",
      "  - positive\\3.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.941)\n",
      "    Vote predicted:      positive (0.918)\n",
      "  - positive\\48.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.853)\n",
      "    Vote predicted:      positive (0.986)\n",
      "  - positive\\49.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.949)\n",
      "    Vote predicted:      positive (0.985)\n",
      "  - positive\\70.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.966)\n",
      "    Vote predicted:      positive (0.985)\n",
      "  - positive\\88.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.667)\n",
      "    Vote predicted:      positive (0.979)\n",
      "  - positive\\96.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.712)\n",
      "    Vote predicted:      positive (0.989)\n",
      "\n",
      "Both WRONG (15):\n",
      "  - negative\\629.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.999)\n",
      "    Vote predicted:      positive (0.990)\n",
      "  - negative\\631.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.995)\n",
      "    Vote predicted:      positive (0.964)\n",
      "  - negative\\732.jpg (GT: negative)\n",
      "    Classical predicted: positive (1.000)\n",
      "    Vote predicted:      positive (0.893)\n",
      "  - negative\\734.jpg (GT: negative)\n",
      "    Classical predicted: positive (1.000)\n",
      "    Vote predicted:      positive (0.981)\n",
      "  - negative\\753.jpg (GT: negative)\n",
      "    Classical predicted: positive (0.979)\n",
      "    Vote predicted:      positive (0.710)\n",
      "  - positive\\123 (21).jpg (GT: positive)\n",
      "    Classical predicted: negative (0.973)\n",
      "    Vote predicted:      negative (0.980)\n",
      "  - positive\\123 (25).jpg (GT: positive)\n",
      "    Classical predicted: negative (0.998)\n",
      "    Vote predicted:      negative (0.980)\n",
      "  - positive\\13.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.953)\n",
      "    Vote predicted:      negative (0.973)\n",
      "  - positive\\31.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.995)\n",
      "    Vote predicted:      negative (0.979)\n",
      "  - positive\\52.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.992)\n",
      "    Vote predicted:      negative (0.980)\n",
      "  - positive\\59.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.997)\n",
      "    Vote predicted:      negative (0.924)\n",
      "  - positive\\85.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.984)\n",
      "    Vote predicted:      negative (0.978)\n",
      "  - positive\\91.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.982)\n",
      "    Vote predicted:      negative (0.977)\n",
      "  - positive\\95.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.995)\n",
      "    Vote predicted:      negative (0.829)\n",
      "  - positive\\98.jpg (GT: positive)\n",
      "    Classical predicted: negative (0.883)\n",
      "    Vote predicted:      negative (0.975)\n",
      "\n",
      "========================================\n",
      "       END OF REPORT\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
