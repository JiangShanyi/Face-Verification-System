{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35379cb",
   "metadata": {},
   "source": [
    "## Noise Resilience Benchmarking of Hybrid Quantum-Classical Face Verification Under NISQ Constraints​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2567911",
   "metadata": {},
   "source": [
    "### Step 1：Image Filtering - from preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8f9adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & config ---\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "ROOT = Path(\"./\")\n",
    "DATA_A = ROOT / \"data\" / \"Positive\"\n",
    "DATA_B = ROOT / \"data\" / \"Negative\"\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91941a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Negative', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "train_root = ROOT / \"_bin_dataset\" / \"train\"\n",
    "val_root   = ROOT / \"_bin_dataset\" / \"val\"\n",
    "\n",
    "def build_split(srcA, srcB, train_ratio=0.8, limit_per_class=300):\n",
    "    import shutil\n",
    "    for p in [train_root, val_root]:\n",
    "        if p.exists():\n",
    "            shutil.rmtree(p)\n",
    "    for p in [train_root/srcA.name, train_root/srcB.name, val_root/srcA.name, val_root/srcB.name]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def list_imgs(d: Path):\n",
    "        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}\n",
    "        return [p for p in d.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "    A = list_imgs(srcA)\n",
    "    B = list_imgs(srcB)\n",
    "    random.shuffle(A); random.shuffle(B)\n",
    "    A = A[:min(limit_per_class, len(A))]\n",
    "    B = B[:min(limit_per_class, len(B))]\n",
    "\n",
    "    kA = int(len(A)*train_ratio)\n",
    "    kB = int(len(B)*train_ratio)\n",
    "\n",
    "    for src in A[:kA]: shutil.copy(src, train_root/srcA.name/src.name)\n",
    "    for src in A[kA:]: shutil.copy(src, val_root/srcA.name/src.name)\n",
    "    for src in B[:kB]: shutil.copy(src, train_root/srcB.name/src.name)\n",
    "    for src in B[kB:]: shutil.copy(src, val_root/srcB.name/src.name)\n",
    "\n",
    "build_split(DATA_A, DATA_B)\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_root, transform=tfm)\n",
    "val_ds   = datasets.ImageFolder(val_root,   transform=tfm)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_backbone = ROOT / \"outputs\" / \"resnet18_backbone_only.pt\"\n",
    "if not ckpt_backbone.exists():\n",
    "    alt = ROOT / \"resnet18_finetuned.pt\"\n",
    "    assert alt.exists(), \"\"\n",
    "    ckpt_backbone = alt\n",
    "\n",
    "backbone = resnet18(weights=None)\n",
    "state = torch.load(ckpt_backbone, map_location=\"cpu\")\n",
    "if isinstance(state, dict) and \"state_dict\" in state:\n",
    "    state = state[\"state_dict\"]\n",
    "_ = backbone.load_state_dict(state, strict=False)\n",
    "\n",
    "in_feats = backbone.fc.in_features  # 512\n",
    "backbone.fc = nn.Identity()\n",
    "backbone.to(device).eval()\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5acac",
   "metadata": {},
   "source": [
    "## L512-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe50ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L512to4(nn.Module):\n",
    "    def __init__(self, in_dim=512, hidden_dim=4):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, hidden_dim)  # W: [4×512], b: [4]\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):  # z: [B,512]\n",
    "        return self.act(self.fc(z))  # [B,4]\n",
    "\n",
    "proj = L512to4(in_dim=in_feats, hidden_dim=4).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85465ac",
   "metadata": {},
   "source": [
    "## Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "n_qubits = 4\n",
    "n_layers = 6\n",
    "\n",
    "# Noise-Free\n",
    "#dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "# Noise case\n",
    "dev = qml.device(\"default.mixed\", wires=n_qubits)\n",
    "\n",
    "def entangle_ladder():\n",
    "\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "\n",
    "dev = qml.device(\"default.mixed\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_block(x, weights,\n",
    "                  noise_type=\"none\",\n",
    "                  gamma_T1=0.0, \n",
    "                  gamma_T2=0.0,\n",
    "                  epsilon=0.0):\n",
    "    \"\"\"\n",
    "    noise_type: \"none\", \"T1\", \"T2\", \"rotation\"\n",
    "    gamma_T1: amplitude damping parameter (0~1)\n",
    "    gamma_T2: dephasing parameter (0~1)\n",
    "    epsilon: rotation over-rotation (radians)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    #   Helper: inject noise\n",
    "    # --------------------------\n",
    "    def apply_noise():\n",
    "        if noise_type == \"T1\":\n",
    "            for q in range(n_qubits):\n",
    "                qml.AmplitudeDamping(gamma_T1, wires=q)\n",
    "\n",
    "        elif noise_type == \"T2\":\n",
    "            for q in range(n_qubits):\n",
    "                qml.PhaseDamping(gamma_T2, wires=q)\n",
    "\n",
    "        elif noise_type == \"T12\":\n",
    "            for q in range(n_qubits):\n",
    "                qml.AmplitudeDamping(gamma_T1, wires=q)\n",
    "                qml.PhaseDamping(gamma_T2, wires=q)\n",
    "\n",
    "    for q in range(n_qubits):\n",
    "        qml.Hadamard(wires=q)\n",
    "        qml.RY(np.pi * x[q] / 2, wires=q)\n",
    "\n",
    "    # initial layer noise\n",
    "    apply_noise()\n",
    "\n",
    "    for l in range(n_layers):\n",
    "\n",
    "        for q in range(n_qubits):\n",
    "\n",
    "            if noise_type == \"rotation\":\n",
    "                qml.RY(weights[l, q] + epsilon, wires=q)\n",
    "            else:\n",
    "                qml.RY(weights[l, q], wires=q)\n",
    "\n",
    "        # layer noise\n",
    "        apply_noise()\n",
    "        entangle_ladder()\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# QuantumLayer: input [B,4] → output [B,4]\n",
    "#############################################\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, noise_type=\"none\", T1=0.0, T2=0.0, epsilon=0.0):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits))\n",
    "\n",
    "        self.noise_type = noise_type\n",
    "        self.gamma_T1 = T1\n",
    "        self.gamma_T2 = T2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    def forward(self, x4_batch):\n",
    "        outs = []\n",
    "        for i in range(x4_batch.shape[0]):\n",
    "            y = quantum_block(\n",
    "                    x4_batch[i],\n",
    "                    self.weights,\n",
    "                    noise_type=self.noise_type,\n",
    "                    gamma_T1=self.gamma_T1,\n",
    "                    gamma_T2=self.gamma_T2,\n",
    "                    epsilon=self.epsilon\n",
    "            )\n",
    "            y = torch.stack(y)\n",
    "            outs.append(y)\n",
    "\n",
    "        return torch.stack(outs).float()\n",
    "\n",
    "\n",
    "# q_layer = QuantumLayer(noise_type='None').to(device)\n",
    "# q_layer = QuantumLayer(noise_type=\"T1\", T1=0.02).to(device)\n",
    "q_layer = QuantumLayer(noise_type=\"T2\", T2=0.049).to(device)\n",
    "# q_layer = QuantumLayer(noise_type=\"rotation\", epsilon=0.03).to(device)\n",
    "# q_layer = QuantumLayer(noise_type=\"T12\", T1=0.02, T2=0.049).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224b8a0",
   "metadata": {},
   "source": [
    "## L4-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L4to2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(4, 2)\n",
    "    def forward(self, z4):\n",
    "        return self.fc(z4)\n",
    "\n",
    "head = L4to2().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037fac0",
   "metadata": {},
   "source": [
    "## L+Q+L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Hybrid model = backbone → L512→4 → Q → L4→2\n",
    "#############################################\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, backbone, proj, q_layer, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = proj\n",
    "        self.q_layer = q_layer\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        with torch.no_grad():\n",
    "            z512 = self.backbone(imgs)\n",
    "\n",
    "        x4 = self.proj(z512)            # [B,4]\n",
    "        zq = self.q_layer(x4)           # [B,4], quantum output\n",
    "        logits = self.head(zq)          # [B,2]\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = HybridModel(backbone, proj, q_layer, head).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f022e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train 0.3837/0.929 | Val 0.2991/0.983\n",
      "[2] Train 0.2882/0.975 | Val 0.2796/0.975\n",
      "[3] Train 0.2638/0.979 | Val 0.2608/0.975\n",
      "[4] Train 0.2423/0.983 | Val 0.2418/0.975\n",
      "[5] Train 0.2291/0.981 | Val 0.2303/0.975\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Training\n",
    "#############################################\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": proj.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": q_layer.parameters(), \"lr\": 1e-2},\n",
    "    {\"params\": head.parameters(), \"lr\": 1e-3},\n",
    "])\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logits = model(imgs)  # backbone → L512→4 → Q → L4→2\n",
    "\n",
    "        loss = crit(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 统计\n",
    "        loss_sum += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "# ----------------- Run training -----------------\n",
    "for ep in range(1, 6):\n",
    "    trL, trA = run_epoch(train_loader, True)\n",
    "    vaL, vaA = run_epoch(val_loader, False)\n",
    "    print(f\"[{ep}] Train {trL:.4f}/{trA:.3f} | Val {vaL:.4f}/{vaA:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97474f",
   "metadata": {},
   "source": [
    "## Test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8e161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_class': 'Positive',\n",
       " 'prob_negative': 0.18856589496135712,\n",
       " 'prob_positive': 0.8114340901374817,\n",
       " 'is_taylor': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_image(model, image_path, device=\"cpu\"):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_t = tfm(img).unsqueeze(0).to(device)   # shape [1,3,224,224]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_t)                 # [1,2]\n",
    "        probs = F.softmax(logits, dim=1)[0]   # [2]\n",
    "        pred  = torch.argmax(probs).item()\n",
    "\n",
    "    class_names = train_ds.classes  # [Negative, Positive]\n",
    "    \n",
    "    return {\n",
    "        \"pred_class\": class_names[pred],\n",
    "        \"prob_negative\": float(probs[0]),\n",
    "        \"prob_positive\": float(probs[1]),\n",
    "        \"is_taylor\": class_names[pred] == \"Positive\",\n",
    "    }\n",
    "\n",
    "test_img = ROOT / \"test1\" / \"7.jpg\"\n",
    "\n",
    "result = predict_image(model, test_img, device)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64802405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final-test classes: ['negative', 'positive']\n",
      "\n",
      "[final_test] Overall accuracy: 0.9235\n",
      "Class 'negative': acc = 0.9955  (n = 220)\n",
      "Class 'positive': acc = 0.7917  (n = 120)\n"
     ]
    }
   ],
   "source": [
    "# T2\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "final_root = ROOT / \"final_test_full\"\n",
    "\n",
    "# ImageFolder will use the subfolder names as class labels\n",
    "final_ds = datasets.ImageFolder(final_root, transform=tfm)\n",
    "final_loader = DataLoader(final_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Final-test classes:\", final_ds.classes)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in final_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Hybrid model: backbone → L512→4 → QuantumLayer(with noise) → L4→2\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_acc = (all_preds == all_labels).mean()\n",
    "print(f\"\\n[final_test] Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "# Per-class accuracy (for 'negative' and 'positive')\n",
    "for idx, name in enumerate(final_ds.classes):\n",
    "    mask = (all_labels == idx)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    cls_acc = (all_preds[mask] == idx).mean()\n",
    "    print(f\"Class '{name}': acc = {cls_acc:.4f}  (n = {mask.sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb86c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final-test classes: ['negative', 'positive']\n",
      "\n",
      "[final_test] Overall accuracy: 0.9324\n",
      "Class 'negative': acc = 0.9864  (n = 220)\n",
      "Class 'positive': acc = 0.8333  (n = 120)\n"
     ]
    }
   ],
   "source": [
    "# T1\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "final_root = ROOT / \"final_test_full\"\n",
    "\n",
    "# ImageFolder will use the subfolder names as class labels\n",
    "final_ds = datasets.ImageFolder(final_root, transform=tfm)\n",
    "final_loader = DataLoader(final_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Final-test classes:\", final_ds.classes)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in final_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Hybrid model: backbone → L512→4 → QuantumLayer(with noise) → L4→2\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_acc = (all_preds == all_labels).mean()\n",
    "print(f\"\\n[final_test] Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "# Per-class accuracy (for 'negative' and 'positive')\n",
    "for idx, name in enumerate(final_ds.classes):\n",
    "    mask = (all_labels == idx)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    cls_acc = (all_preds[mask] == idx).mean()\n",
    "    print(f\"Class '{name}': acc = {cls_acc:.4f}  (n = {mask.sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d64dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final-test classes: ['negative', 'positive']\n",
      "\n",
      "[final_test] Overall accuracy: 0.9294\n",
      "Class 'negative': acc = 0.9818  (n = 220)\n",
      "Class 'positive': acc = 0.8333  (n = 120)\n"
     ]
    }
   ],
   "source": [
    "# Rot\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "final_root = ROOT / \"final_test_full\"\n",
    "\n",
    "# ImageFolder will use the subfolder names as class labels\n",
    "final_ds = datasets.ImageFolder(final_root, transform=tfm)\n",
    "final_loader = DataLoader(final_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Final-test classes:\", final_ds.classes)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in final_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Hybrid model: backbone → L512→4 → QuantumLayer(with noise) → L4→2\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_acc = (all_preds == all_labels).mean()\n",
    "print(f\"\\n[final_test] Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "# Per-class accuracy (for 'negative' and 'positive')\n",
    "for idx, name in enumerate(final_ds.classes):\n",
    "    mask = (all_labels == idx)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    cls_acc = (all_preds[mask] == idx).mean()\n",
    "    print(f\"Class '{name}': acc = {cls_acc:.4f}  (n = {mask.sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ac74486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final-test classes: ['negative', 'positive']\n",
      "\n",
      "[final_test] Overall accuracy: 0.9235\n",
      "Class 'negative': acc = 0.9955  (n = 220)\n",
      "Class 'positive': acc = 0.7917  (n = 120)\n"
     ]
    }
   ],
   "source": [
    "# T12\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "final_root = ROOT / \"final_test_full\"\n",
    "\n",
    "# ImageFolder will use the subfolder names as class labels\n",
    "final_ds = datasets.ImageFolder(final_root, transform=tfm)\n",
    "final_loader = DataLoader(final_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Final-test classes:\", final_ds.classes)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in final_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Hybrid model: backbone → L512→4 → QuantumLayer(with noise) → L4→2\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_acc = (all_preds == all_labels).mean()\n",
    "print(f\"\\n[final_test] Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "# Per-class accuracy (for 'negative' and 'positive')\n",
    "for idx, name in enumerate(final_ds.classes):\n",
    "    mask = (all_labels == idx)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    cls_acc = (all_preds[mask] == idx).mean()\n",
    "    print(f\"Class '{name}': acc = {cls_acc:.4f}  (n = {mask.sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e2fd74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: outputs\\noise_models\\hybrid_noise_model_T2.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = ROOT / \"outputs\" / \"noise_models\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give it a meaningful name, e.g. include noise type\n",
    "ckpt_path = CKPT_DIR / \"hybrid_noise_model_T2.pt\"\n",
    "print(\"Checkpoint path:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7957297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved noisy model to outputs\\noise_models\\hybrid_noise_model_T2.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell: save the trained noisy HybridModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# If you tracked best_epoch / best_val_acc in training, you can also include them\n",
    "save_obj = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    # Optional extra info:\n",
    "    # \"epoch\": best_epoch,\n",
    "    # \"val_acc\": best_val_acc,\n",
    "    # \"noise_type\": q_layer.noise_type,\n",
    "}\n",
    "\n",
    "torch.save(save_obj, ckpt_path)\n",
    "print(f\"Saved noisy model to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f313ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: outputs\\noise_models\\hybrid_noise_model_T1.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = ROOT / \"outputs\" / \"noise_models\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give it a meaningful name, e.g. include noise type\n",
    "ckpt_path = CKPT_DIR / \"hybrid_noise_model_T1.pt\"\n",
    "print(\"Checkpoint path:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved noisy model to outputs\\noise_models\\hybrid_noise_model_T1.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell: save the trained noisy HybridModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# If you tracked best_epoch / best_val_acc in training, you can also include them\n",
    "save_obj = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    # Optional extra info:\n",
    "    # \"epoch\": best_epoch,\n",
    "    # \"val_acc\": best_val_acc,\n",
    "    # \"noise_type\": q_layer.noise_type,\n",
    "}\n",
    "\n",
    "torch.save(save_obj, ckpt_path)\n",
    "print(f\"Saved noisy model to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f0f50f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: outputs\\noise_models\\hybrid_noise_model_rot.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = ROOT / \"outputs\" / \"noise_models\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give it a meaningful name, e.g. include noise type\n",
    "ckpt_path = CKPT_DIR / \"hybrid_noise_model_rot.pt\"\n",
    "print(\"Checkpoint path:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565406ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved noisy model to outputs\\noise_models\\hybrid_noise_model_rot.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell: save the trained noisy HybridModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# If you tracked best_epoch / best_val_acc in training, you can also include them\n",
    "save_obj = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    # Optional extra info:\n",
    "    # \"epoch\": best_epoch,\n",
    "    # \"val_acc\": best_val_acc,\n",
    "    # \"noise_type\": q_layer.noise_type,\n",
    "}\n",
    "\n",
    "torch.save(save_obj, ckpt_path)\n",
    "print(f\"Saved noisy model to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f142bffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: outputs\\noise_models\\hybrid_noise_model_T12.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = ROOT / \"outputs\" / \"noise_models\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give it a meaningful name, e.g. include noise type\n",
    "ckpt_path = CKPT_DIR / \"hybrid_noise_model_T12.pt\"\n",
    "print(\"Checkpoint path:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6fd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved noisy model to outputs\\noise_models\\hybrid_noise_model_T12.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell: save the trained noisy HybridModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# If you tracked best_epoch / best_val_acc in training, you can also include them\n",
    "save_obj = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    # Optional extra info:\n",
    "    # \"epoch\": best_epoch,\n",
    "    # \"val_acc\": best_val_acc,\n",
    "    # \"noise_type\": q_layer.noise_type,\n",
    "}\n",
    "\n",
    "torch.save(save_obj, ckpt_path)\n",
    "print(f\"Saved noisy model to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed725e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = ROOT / \"outputs\" / \"noise_models\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give it a meaningful name, e.g. include noise type\n",
    "ckpt_path = CKPT_DIR / \"hybrid_noise_model_T12r.pt\"\n",
    "print(\"Checkpoint path:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dafdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: save the trained noisy HybridModel\n",
    "\n",
    "import torch\n",
    "\n",
    "# If you tracked best_epoch / best_val_acc in training, you can also include them\n",
    "save_obj = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    # Optional extra info:\n",
    "    # \"epoch\": best_epoch,\n",
    "    # \"val_acc\": best_val_acc,\n",
    "    # \"noise_type\": q_layer.noise_type,\n",
    "}\n",
    "\n",
    "torch.save(save_obj, ckpt_path)\n",
    "print(f\"Saved noisy model to {ckpt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
